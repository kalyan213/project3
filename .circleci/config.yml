version: 2.1
orbs:
 slack: circleci/slack@4.10.1
commands:
  destroy:
    parameters:
      workflow_id:
        type: string
    steps:
      - run:
          name: Destroy environments
          when: on_fail
          command: |
            echo "Destroying environment: <<parameters.workflow_id>> "
            aws cloudformation delete-stack --stack-name udapeople-backend-<<parameters.workflow_id>>
            aws s3 rm s3://udapeople-<<parameters.workflow_id>> --recursive
            aws cloudformation delete-stack --stack-name udapeople-frontend-<<parameters.workflow_id>>
  revert_migrations:
    description: Revert the last migration if successfully run in the current workflow.
    steps:
      - run:
          name: Revert migrations
          when: on_fail
          command: |
            cd ~/project3/backend
            if [[ -f "~/project3/.circleci/outputs/db_migration_success" ]]; then
              npm i
              npm run migration:revert
            fi       
          # command: |
          #   ansible-playbook -i inventory.txt playbook.yml
  undo_migrations:
    description: Revert the last migration if successfully run in the current workflow.
    steps:
      - run:
          name: Revert migrations
          when: on_fail
          command: |
            cd ~/project/backend
            if [[ -f "~/project/.circleci/outputs/db_migration_success" ]]; then
              npm i
              npm run migration:revert
            fi
          name: Configure server
          command: |
            ansible-playbook -i inventory.txt playbook.yml
  revert-migration:
    description: Revert the latest migration
    steps:
      - run:
          name: Get the public DNS of EC2 instance from https://memstash.io/
          command: |
            PUBLIC_DNS=$(curl -H "token: kalyan01" --request GET https://api.memstash.io/values/public_dns)
            echo ${PUBLIC_DNS}
            cd .circleci/ansible/
            echo "[all]" > ./inventory
            echo ${PUBLIC_DNS} >> ./inventory
            cat ./inventory
          when: on_fail
      - add_ssh_keys:
          fingerprints: ["33:c8:64:fb:aa:80:1b:63:c6:98:30:9a:c8:8c:9a:14"]
      - run:
          name: Revert the last migration
          command: |
            printenv >> ./backend/.env
            cd .circleci/ansible/
            ansible-playbook -i ./inventory db_rollback.yml
          when: on_fail            
jobs:
  build-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-build]
      - run:
          name: Build front-end
          command: |
            cd frontend
            npm i
            npm run build
            npm audit fix --audit-level=critical --force
      - save_cache:
          paths: [frontend/node_modules]
          key: frontend-build

  build-backend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [backend-build]
      - run:
          name: Back-end build
          command: |
            cd backend
            npm i
            npm run build
            npm audit fix --audit-level=critical --force
      - save_cache:
          paths: [backend/node_modules]
          key: backend-build

  test-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-build]
      - run:
          name: front-end test
          command: |
            cd frontend
            npm i
            npm run test
            npm audit fix --audit-level=critical --force                
  test-backend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [backend-build]
      - run:
          name: back-end test
          command: |
            cd backend
            npm i
            npm run test
            npm audit fix --audit-level=critical --force            
  scan-frontend:
    docker:
      - image: circleci/node:13.8.0      
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-build]
      - run:
          name: front-end scan
          command: |
            cd frontend
            npm i
            npm audit fix --audit-level=critical --force
  scan-backend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [backend-build]
      - run:
          name: back-end scan
          command: |
            cd backend
            npm audit fix --audit-level=critical --force

  deploy-infrastructure:
    docker:
      - image: amazon/aws-cli      
    steps:
      - checkout          
      - run:
          name: Ensure back-end infrastructure exists
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/backend.yml \
              --tags project=udapeople_B \
              --stack-name "udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"
      - run:
          name: Ensure front-end infrastructure exist
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/frontend.yml \
              --tags project=udapeople_F \
              --stack-name "udapeople-frontend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"
      - run:
          name: Add back-end ip to ansible inventory
          command: |
            aws cloudformation describe-stacks \
            --stack-name "udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}" \
            --query "Stacks[0].Outputs[0].OutputValue" \
            --output text >> .circleci/ansible/inventory.txt
            cat .circleci/outputs/backend_url.txt
      - run: yum -y install tar gzip
      - persist_to_workspace:
          root: ~/
          paths:
            - project/.circleci/outputs/backend_url.txt
            - project/.circleci/ansible/inventory.txt
      - destroy:
          workflow_id: '${CIRCLE_WORKFLOW_ID:0:7}'

  deploy-backend:
    docker:
      - image: python:3.7-alpine3.11
    steps:
      - checkout
      - attach_workspace:
          at: ~/
      - run:
          name: Install dependencies
          command: |
            apk add --update curl nodejs npm ansible zip
            pip install awscli
      - run:
          name: Build backend app
          command: |
            cd ~/project/backend
            npm i
            npm run build
            cd dist
            cp ../package.json .
            zip -rq dist.zip *
            mkdir -p ~/project/.circleci/ansible/roles/deploy_backend/files/
            mv dist.zip ~/project/.circleci/ansible/roles/deploy_backend/files/
      - add_ssh_keys:
          fingerprints: [ "dd:ec:a5:7d:8e:89:75:7c:cd:15:2a:5d:96:e5:5a:c8" ]
      - run:
          name: Setup EC2 instance & copy compiled backend to the EC2 instance
          command: |
            ls ~/project/.circleci/ansible/roles/deploy_backend/files/
            cd ~/project/backend
            ansible-galaxy install weareinteractive.environment
            ANSIBLE_HOST_KEY_CHECKING=False ansible-playbook -i ~/project/.circleci/ansible/inventory.txt ~/project/.circleci/ansible/playbook.yml
      - destroy_environment:
          workflow_id: "${CIRCLE_WORKFLOW_ID:0:7}"
      - undo_migrations
  
    # docker:
    #   - image: python:3.7-alpine3.11
    #     environment:
    #       NODE_ENV: "production"
    #       VERSION: "1"
    #       ENVIRONMENT: "production"
    #       TYPEORM_CONNECTION: $TYPEORM_CONNECTION
    #       TYPEORM_HOST: $TYPEORM_HOST
    #       TYPEORM_USERNAME: $TYPEORM_USERNAME
    #       TYPEORM_PASSWORD: $TYPEORM_PASSWORD
    #       TYPEORM_DATABASE: $TYPEORM_DATABASE
    #       TYPEORM_PORT: $TYPEORM_PORT
    #       TYPEORM_ENTITIES: $TYPEORM_ENTITIES    
    # steps:
    #   - checkout
    #   - add_ssh_keys:
    #       fingerprints: ["dd:ec:a5:7d:8e:89:75:7c:cd:15:2a:5d:96:e5:5a:c8"]
    #   - attach_workspace:
    #       at: ~/
    #   - restore_cache:
    #       keys: [build-backend]
    #   - run:
    #       name: Install dependencies
    #       command: |
    #         apk add --no-cache curl
    #         apk add --no-cache --upgrade bash
    #         apk add --no-cache --update ansible
    #         apk add --no-cache openssh-client
    #         apk add --no-cache tar
    #         apk add --no-cache gzip
    #         apk add --no-cache nodejs
    #         apk add --no-cache npm
    #         apk add --no-cache jq
    #         pip3 install awscli
    #   - run:
    #       name: Get the public DNS of EC2 instance from https://memstash.io/
    #       command: |
    #         PUBLIC_DNS=$(curl -H "token: kalyan02" --request GET https://api.memstash.io/values/public_dns)
    #         echo ${PUBLIC_DNS}
    #         cd .circleci/ansible/
    #         echo "[all]" > ./inventory
    #         echo ${PUBLIC_DNS} >> ./inventory
    #         cat ./inventory
    #   - run:
    #       name: Configure server
    #       command: |
    #         printenv > ./backend/.env
    #         cd .circleci/ansible/
    #         ansible-playbook -i ./inventory deploy-backend.yml
    #   - revert-migration
    #   - destroy

  deploy-frontend:
    docker:
      - image: python:3.7-alpine3.11
    steps:
      - checkout
      - run:
          name: Install system dependencies
          command: |
            apk add --no-cache curl
            apk add --no-cache --upgrade bash
            apk add --no-cache --update npm
            pip3 install awscli
      - run:
          name: Build the frontend
          command: |
            PUBLIC_DNS=$(curl -H "token: kalyan02" --request GET https://api.memstash.io/values/public_dns)
            echo ${PUBLIC_DNS}
            export API_URL="http://${PUBLIC_DNS}:3030"
            echo API_URL=${API_URL}
            cd frontend
            npm install
            npm run build
      - run:
          name: Copy built frontend files to the S3 bucket
          command: |
            aws s3 cp ./frontend/dist s3://udapeople-kalyan02/ --recursive
      - destroy

  smoke-test:
    docker:
      - image: python:3.7-alpine3.11
    steps:
      - checkout
      - run:
          name: Install system dependencies
          command: |
            apk add --no-cache curl
            apk add --no-cache --upgrade bash
            apk add --no-cache --update ansible
            apk add --no-cache openssh-client
            pip3 install awscli
      - run:
          name: Smoke test on frontend
          command: |
            URL="http://udapeople-kalyan02.s3-website.us-east-2.amazonaws.com/#/employees"
            if curl -s ${URL} | grep "Welcome"
            then
              return 1
            else
              return 0
            fi
      - run:
          name: Smoke test on backend
          command: |
            PUBLIC_DNS=$(curl -H "token: kalyan02" --request GET https://api.memstash.io/values/public_dns)
            echo ${PUBLIC_DNS}
            if curl -s "http://${PUBLIC_DNS}:3030/api/status" | grep "ok"
            then
              return 0
            else
              return 1
            fi
      - revert-migration
      - destroy
  
  cloudfront-update:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Update cloudfront distribution
          command: |
            aws cloudformation deploy \
              --stack-name udapeople-cloudfront \
              --template-file .circleci/files/cloudfront.yml \
              --region ${AWS_DEFAULT_REGION} \
              --parameter-overrides WorkflowID="kalyan02"
      - destroy
  cleanup:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Get old stack workflow id
          command: |
            OLD_WORKFLOW_ID=$(aws cloudformation list-exports --query "Exports[?Name=='CircleCI-WorkflowID'].Value" --output text)
            echo OLD_WORKFLOW_ID=${OLD_WORKFLOW_ID}
            STACKS=$(aws cloudformation list-stacks --query "StackSummaries[*].StackName" --stack-status-filter UPDATE_COMPLETE CREATE_COMPLETE --output text)
            echo STACKS=${STACKS}
            echo "Remove old stacks and files"
            if [ -n "${OLD_WORKFLOW_ID}" ] && [[ "${STACKS[@]}" =~ "${OldWorkflowID}" ]]
            then
                echo deleting all files at S3 bucket udapeople-${OLD_WORKFLOW_ID}
                aws s3 rm s3://udapeople-${OLD_WORKFLOW_ID}/ --recursive
                echo deleting stack udapeople-kalyan02
                aws cloudformation delete-stack --stack-name udapeople-kalyan02
                echo deleting stack udapeopleB
                aws cloudformation delete-stack --stack-name udapeople
              fi
      - destroy

workflows:
  default:
    jobs:
      # - build-frontend
      # - build-backend
      # - test-frontend:
      #     requires: [build-frontend]
      # - test-backend:
      #     requires: [build-backend]
      # - scan-backend:
      #     requires: [build-backend]
      # - scan-frontend:
      #     requires: [build-frontend]
      - deploy-infrastructure
          # requires: [test-frontend, test-backend, scan-frontend, scan-backend]
          # filters:
          #   branches:
          #     only: master
      # - deploy-backend:
      #     requires: [deploy-infrastructure]
      #     filters:
      #       branches:
      ##         only: master
